# -*- coding: utf-8 -*-
"""Streamlit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ikP3SmpMovrl9LzyRWgJl3yz5T9-DgMm
"""

from operator import index
import streamlit as st
from pandas.core.frame import DataFrame
import plotly.express as px
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

from random import shuffle

import plotly.graph_objs as go
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
from gsheetsdb import connect
import plotly
import colorlover as cl
import plotly.offline as py
import plotly.graph_objs as go

import seaborn as sns

Antonym_list = ['commitment rejection', 'manager worker', 'feminine masculine', 'globally locally',
                'family work', 'stakeholders spectators', 'discrimination impartial', 'challenge obscurity',
                'seasonal temporary',
                'alliance proprietorship', 'public private', 'details outlines', 'responsible neglect',
                'marketing secret', 'trust mistrust', 'independent dependent', 'integrity corruption',
                'bankruptcy prosperity', 'insiders outsiders', 'teamwork individualism', 'foreigners natives',
                'criminal rightful', 'strategic impulsive', 'environment pollution', 'diversity uniformity',
                'progressive conservative', 'salary goodies', 'innovator follower', 'pressure relax',
                'secure risky', 'remote physical', 'sustainable unsustainable', 'product service',
                'essential luxury', 'digital analogue', 'effortless demanding', 'nurture neglect',
                'professional amateur', 'ambiguity clarity', 'credible deceptive', 'widespread local',
                'freedom captive', 'order disorder',
                'goal task', 'cost revenue', 'demand supply', 'opportunity threat', 'flexible rigid',
                'isolating social', 'international local', 'innovative traditional', 'satisfied unsatisfied',
                'solution problem', 'store online', 'loss profit', 'ethical unethical',
                'beneficial harmful', 'economic overpriced', 'outdated modern', 'transparency obscurity',
                'lease sell', 'technical natural', 'consistent inconsistent', 'growth decline',
                'tangible intangible', 'employees consultant', 'financial artisanal', 'child childless',
                'connected disconnected', 'corporate individual']

st.title("Word Embeddings for Business Entities")
# option = st.sidebar.selectbox('Create your own polar pairs?',('Yes',  'No'))

check = st.sidebar.selectbox('Check for', ('Bias', 'Hofstede'))


def polar_list(list):
    right_polar_list = []
    left_polar_list = []
    for i in range(0, len(list)):
        left_polar_list.append(list[i][0])
        right_polar_list.append(list[i][1])

    return left_polar_list, right_polar_list


def alphabetical_list_creation(list):
    new_list = []

    for i in range(0, len(list)):

        if list[i][0] < list[i][1]:
            val = list[i][0] + "-" + list[i][1]
            new_list.append(val)

        else:
            val = list[i][1] + "-" + list[i][0]
            new_list.append(val)

    return new_list

def company_count(company_df,input_list,polar_embedding):

  # we then find the number of companies grouped on the basis of location
  for i in input_list:
    print(i)
    j = i.replace("-","")
    subset_df2 = polar_embedding[polar_embedding[j] < 0]
    company_inclined_to_left_polar_df1 = subset_df2['Location'].value_counts()
    left_polar = i.split("-")[0]
    company_inclined_to_left_polar_df1 = pd.DataFrame({'Country':company_inclined_to_left_polar_df1.index, left_polar :company_inclined_to_left_polar_df1.values})
    company_df=pd.merge(company_df, company_inclined_to_left_polar_df1, how='left',on='Country')
    company_df[left_polar] = round( company_df[left_polar] / company_df.iloc[:,1] * 100)

    subset_df1 = polar_embedding[polar_embedding[j] > 0]
    company_inclined_to_right_polar_df1 = subset_df1['Location'].value_counts()
    right_polar = i.split("-")[1]
    company_inclined_to_right_polar_df1 = pd.DataFrame({'Country':company_inclined_to_right_polar_df1.index, right_polar :company_inclined_to_right_polar_df1.values})
    company_df=pd.merge(company_df, company_inclined_to_right_polar_df1, how='left',on='Country')
    company_df[right_polar] = round( company_df[right_polar] / company_df.iloc[:,1] * 100)


  company_df = company_df.fillna(0)

  # We are considering only the countries if the numberof companies in the country is over 3
  company_df = company_df[company_df['Total Count'] > 3]

  return company_df


def polar_ranking(polar_list, total_score, ranking, company_df):
    total_sum = 0
    total_sum_list = []
    polar_ranking_list = []
    polar_index = 0
    for index, row in company_df.iterrows():

        for i in polar_list:
            total_sum = total_sum + (row[i])
        # print(company_df.iloc[index,2:])
        total_sum_list.append(total_sum / len(polar_list))
        polar_ranking_list.append(index + 1)
        total_sum = 0

    company_df[total_score] = total_sum_list
    company_df = company_df.sort_values(by=[total_score], ascending=False)
    company_df[ranking] = polar_ranking_list

    return company_df

list_powerdistance_random =[('make', 'break'), ('cameraman', 'playwright'), ('mystical', 'factual'), ('promotional', 'defamation'), ('iconic', 'unknown')]
list_individualism_random = [('lop', 'secure'), ('shah', 'poor'), ('pneumatic', 'solid'), ('interpret', 'misinterpret'), ('confer', 'refuse')]
list_masculinity_random = [('try', 'abstain'), ('fatalistic', 'freewill'), ('knowledgeable', 'uninformed'), ('confine', 'free'), ('fan', 'warm')]
list_longterm_random = [('innovator', 'follower'), ('sensory', 'numb'), ('hedge', 'squander'), ('arachnid', 'serpent'), ('disclose', 'secrete')]
list_indulgence_random = [('diagnose', 'sicken'), ('intercourse', 'disconnection'), ('sensory', 'sensorial'), ('emasculate', 'strengthen'), ('metropolitan', 'rural')]
list_uncertainity_avoidance_random = [('stretcher', 'compressor'), ('amalgamate', 'separate'), ('caretaker', 'assailant'), ('caretaker', 'violator'), ('contaminate', 'sterilize')]


list_powerdistance_similar =[('leaders', 'subordinates'), ('leadership', 'subordinated'), ('party', 'commands'), ('opposition', 'command'), ('led', 'reassigned')]
list_individualism_similar = [('originality', 'communities'), ('uniqueness', 'organizations'), ('creativity', 'neighborhood'), ('individualism', 'outreach'), ('spontaneity', 'local')]
list_masculinity_similar = [('achievements', 'loving'), ('accomplishment', 'nurturing'), ('excellence', 'cared'), ('accomplishments', 'care'), ('award', 'sick')]
list_longterm_similar = [('pragmatism', 'conceptions'), ('constructive', 'empirical'), ('principled', 'foundational'), ('practical', 'contexts'), ('sensible', 'prescriptive')]
list_indulgence_similar = [('visionary', 'disciple'), ('innovators', 'devotee'), ('inventor', 'adherent'), ('entrepreneur', 'followers'), ('pioneering', 'devout')]
list_uncertainity_avoidance_similar = [('simplicity', 'complexities'), ('subtlety', 'sophistication'), ('honesty', 'richness'), ('originality', 'inherent'), ('consistency', 'computational')]

list_powerdistance =[('hierarchical','nonhierarchical'),('superior','equal'),('leader','subordinate'),('inequality','equality'),('autocrat','democrat')]
list_individualism = [('individuality','community'),('self-interest','harmony'),('tasks','relationships'),('individual','groups'),('universalism','particularism')]
list_masculinity = [('achievement', 'support'),('competitive', 'caring'),('assertive', 'submissive'),('ambitious', 'unambitious'),('sucess','cooperation')]
list_longterm = [('pragmatic','normative'),('progress','preserve'),('adapt','conserve'),('developing','stable'),('advance','retain')]
list_indulgence = [('fulfillment','restriction'),('satisfaction','limitation'),('liberty','moderation'),('expand','direct'),('freedom','regulation')]
list_uncertainity_avoidance = [('clarity','complexity'),('clear','ambiguous'),('certain','uncertain'),('uniformity','diversity'),('agreement','variation')]



if (check == 'Bias'):
    company_or_country = st.sidebar.selectbox('Check for', ('Companies', 'Countries'))
    if (company_or_country == 'Countries'):
        antonym_pair = st.sidebar.selectbox("Select the Antonymn pair", Antonym_list)

        antonym_pair = str(antonym_pair.replace(" ", "_"))

        gnews_url = "https://docs.google.com/spreadsheets/d/1wtUfzJOPIuwVPRuOwZ1zU-JFQF5MQ2a_CnQDKGfPOyo/edit?usp=sharing"
        wiki_url = "https://docs.google.com/spreadsheets/d/12mfEh9o9fyop-ChZ1fUt7_5uDK2DNQHkQhsgvpHkiao/edit?usp=sharing"
        twitter_url = "https://docs.google.com/spreadsheets/d/1_0G95RXRVpu1sjrlGsL74yf1pEqStqv9P0DpbWxrCqo/edit?usp=sharing"
        reddit_url = "https://docs.google.com/spreadsheets/d/1c2o_9RhF1j-WO2rXItt8748DXHbM7YRXIuuSRln5scc/edit?usp=sharing"

        conn = connect()
        gnews_rows = conn.execute(f'SELECT * FROM "{gnews_url}"')
        wiki_rows = conn.execute(f'SELECT * FROM "{wiki_url}"')
        twitter_rows = conn.execute(f'SELECT * FROM "{twitter_url}"')
        reddit_rows = conn.execute(f'SELECT * FROM "{reddit_url}"')

        gnews = pd.DataFrame(gnews_rows)
        country = list(gnews['Country'].str.split('_').str[0])
        gnews.set_index('Country', inplace=True)

        wiki = pd.DataFrame(wiki_rows)
        wiki.set_index('Country', inplace=True)

        twitter = pd.DataFrame(twitter_rows)
        twitter.set_index('Country', inplace=True)

        reddit = pd.DataFrame(reddit_rows)
        reddit.set_index('Country', inplace=True)

        # url = 'https://drive.google.com/file/d/1TsqXgNiaJ_uaRnP31ugQCiPF-wo5Gxv7/view?usp=sharing'
        # path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
        # df = pd.read_csv(path)
        # st.write(df)

        # st.write(reddit)

        # st.write(country)

        country = st.sidebar.multiselect('Select Upto 5 countries', country)
        #         st.write(country)

        country_gnews = [i + "_gnews" for i in country]
        country_gnews = gnews.loc[country_gnews]

        country_wiki = [i + "_wiki" for i in country]
        country_wiki = wiki.loc[country_wiki]

        country_reddit = [i + "_reddit" for i in country]
        country_reddit = reddit.loc[country_reddit]

        country_twitter = [i + "_twitter" for i in country]
        country_twitter = twitter.loc[country_twitter]

        trace0 = go.Scatter(
            {
                'x': country_reddit[antonym_pair],
                'y': country,
                'legendgroup': 'Reddit',
                'name': 'Reddit',
                'mode': 'markers',
                'marker': {
                    'color': cl.scales['9']['div']['Spectral'][0],
                    'size': 40,
                }
                # 'text': reddit['Country']
            })

        trace1 = go.Scatter(
            {
                'x': country_wiki[antonym_pair],
                'y': country,
                'legendgroup': 'Wikipedia',
                'name': 'Wikipedia',
                'mode': 'markers',
                'marker': {
                    'color': cl.scales['9']['div']['Spectral'][2],
                    'size': 40
                }
                # 'text': wiki['Country']
            })

        trace2 = go.Scatter(
            {
                'x': country_twitter[antonym_pair],
                'y': country,
                'legendgroup': 'Twitter',
                'name': 'Twitter',
                'mode': 'markers',
                'marker': {
                    'color': cl.scales['9']['div']['Spectral'][8],
                    'size': 40
                }
                # 'text': twitter['Country']
            })

        trace3 = go.Scatter(
            {
                'x': country_gnews[antonym_pair],
                'y': country,
                'legendgroup': 'Google News',
                'name': 'Google News',
                'mode': 'markers',
                'marker': {
                    'color': cl.scales['9']['div']['Spectral'][6],
                    'size': 40
                }
                # 'text': gnews['Country']
            })

        layout = go.Layout(
            title='Business Entities',
            hovermode='closest',
            xaxis=dict(
                title=antonym_pair
            ),
            yaxis=dict(
                title='Companies'
            ),
            showlegend=True,
            # CENTER = 0
        )

        fig = go.Figure(data=[trace0, trace1, trace2, trace3], layout=layout)
        st.plotly_chart(fig)

    elif (company_or_country == 'Companies'):

        antonym_pair = st.sidebar.selectbox("Select the Antonymn pair", Antonym_list)

        antonym_pair = str(antonym_pair.replace(" ", "_"))

        gnews_url = "https://docs.google.com/spreadsheets/d/1tntSqC-U1e8wL0Tt1k5bw-uc5G717N6Et9QtXJEdMe0/edit?usp=sharing"
        wiki_url = "https://docs.google.com/spreadsheets/d/1fNwrr1dzwot1tiRL7uM34U3qg-epqk1BIyWY4m_Yx9E/edit?usp=sharing"
        twitter_url = "https://docs.google.com/spreadsheets/d/1AMuAYCvOM5bmqMmvP6dCEr9xVqdsqk_MojjG4cJSAjo/edit?usp=sharing"
        reddit_url = "https://docs.google.com/spreadsheets/d/1a5a2yuQ4B_Lq-oO6g6ex0m0gf4UsL4j8_A-Zg3d2uxg/edit?usp=sharing"

        conn = connect()
        gnews_rows = conn.execute(f'SELECT * FROM "{gnews_url}"')
        wiki_rows = conn.execute(f'SELECT * FROM "{wiki_url}"')
        twitter_rows = conn.execute(f'SELECT * FROM "{twitter_url}"')
        reddit_rows = conn.execute(f'SELECT * FROM "{reddit_url}"')

        gnews = pd.DataFrame(gnews_rows)
        company = list(set(gnews['Company_Name']))
        gnews.set_index('Company_Name', inplace=True)

        wiki = pd.DataFrame(wiki_rows)
        wiki.set_index('Company_Name', inplace=True)

        twitter = pd.DataFrame(twitter_rows)
        twitter.set_index('Company_Name', inplace=True)

        reddit = pd.DataFrame(reddit_rows)
        reddit.set_index('Company_Name', inplace=True)

        company = st.sidebar.multiselect('Select Upto 5 companies', company)

        # country_gnews = [i+"_gnews" for i in country]
        company_gnews = gnews.loc[company]

        # country_wiki = [i+"_wiki" for i in country]
        company_wiki = wiki.loc[company]

        # country_reddit = [i+"_reddit" for i in country]
        company_reddit = reddit.loc[company]

        # country_twitter = [i+"_twitter" for i in country]
        company_twitter = twitter.loc[company]

        # reddit = reddit.head(5)
        # wiki = wiki.head(5)
        # twitter = twitter.head(5)
        # gnews = gnews.head(5)

        trace0 = go.Scatter(
            {
                'x': company_reddit[antonym_pair],
                'y': company,
                'legendgroup': 'Reddit',
                'name': 'Reddit',
                'mode': 'markers',
                'marker': {
                    'color': cl.scales['9']['div']['Spectral'][0],
                    'size': 40,
                },
                # 'text': reddit['Country']
            })

        trace1 = go.Scatter(
            {
                'x': company_wiki[antonym_pair],
                'y': company,
                'legendgroup': 'Wikipedia',
                'name': 'Wikipedia',
                'mode': 'markers',
                'marker': {
                    'color': cl.scales['9']['div']['Spectral'][2],
                    'size': 40
                },
                # 'text': wiki['Country']
            })

        trace2 = go.Scatter(
            {
                'x': company_twitter[antonym_pair],
                'y': company,
                'legendgroup': 'Twitter',
                'name': 'Twitter',
                'mode': 'markers',
                'marker': {
                    'color': cl.scales['9']['div']['Spectral'][8],
                    'size': 40
                },
                # 'text': twitter['Country']
            })

        trace3 = go.Scatter(
            {
                'x': company_gnews[antonym_pair],
                'y': company,
                'legendgroup': 'Google News',
                'name': 'Google News',
                'mode': 'markers',
                'marker': {
                    'color': cl.scales['9']['div']['Spectral'][6],
                    'size': 40
                },
                # 'text': gnews['Country']
            })

        layout = go.Layout(
            title='Business Entities',
            hovermode='closest',
            xaxis=dict(
                title=antonym_pair
            ),
            yaxis=dict(
                title='Companies'
            ),
            showlegend=True,
            # CENTER = 0
        )

        fig = go.Figure(data=[trace0, trace1, trace2, trace3], layout=layout)
        st.plotly_chart(fig)

if (check == 'Hofstede'):
    Hofstede_dimensions = st.sidebar.selectbox('Check for', ('Power Distance', 'Individualism vs Collectivism','Masculinity vs Femininity',
                                                             'Long Term vs Short Term Orientation','Indulgence vs Restraint','Uncertainty Avoidance'))

    polar_url = "https: // docs.google.com / spreadsheets / d / 1Tjk72bE1CKWmXnmdrxdAyM0MvmEtWCshINJKRUadAJU / edit?usp = sharing"
    fortune_500_url = "https://docs.google.com/spreadsheets/d/1k1jIYhrfipRQkU4wzsPpoTuar-MvVlFd7my2qA2aOds/edit?usp=sharing"
    hofstede_url = "https://docs.google.com/spreadsheets/d/1EtYAgilX8CGbGZ-dsLW6rNp-Dqn_ikeT4zvhULOfgKM/edit?usp=sharing"

    conn = connect()
    polar_rows = conn.execute(f'SELECT * FROM "{polar_url}"')
    fortune_500_url = conn.execute(f'SELECT * FROM "{fortune_500_url}"')
    hofstede_url = conn.execute(f'SELECT * FROM "{hofstede_url}"')


    new_df = pd.DataFrame(polar_rows)
    fortune_500_company = pd.DataFrame(fortune_500_url)
    hofstede_df = pd.DataFrame(hofstede_url)

    fortune_500_company['Company'] = fortune_500_company['Company'].str.lower()
    fortune_500_company['Company'] = fortune_500_company['Company'].str.replace(" ", "")

    polar_embedding = pd.merge(fortune_500_company, new_df, how="right", left_on="Company", right_on="Unnamed: 0")

    polar_embedding = polar_embedding.drop(['Rank'], axis=1)  # This will drop the column Rank
    polar_embedding = polar_embedding.drop(['Unnamed: 0'], axis=1)  # This will drop the column Rank

    # This will find the total number of companies in our data frame based on Location
    total_company_list_based_on_loc = polar_embedding['Location'].value_counts()
    total_company_count_df = pd.DataFrame({'Country': total_company_list_based_on_loc.index, 'Total Count': total_company_list_based_on_loc.values})
    dim_index = ""
    dim_ranking = ""
    if (Hofstede_dimensions == 'Power Distance'):
        dim_index="Power distance index"
        dim_ranking="Power distance ranking"
        left_polar_list_random, right_polar_list_random = polar_list(list_powerdistance_random)
        left_polar_list_similar, right_polar_list_similar = polar_list(list_powerdistance_similar)
        left_polar_list_human, right_polar_list_human = polar_list(list_powerdistance)

        input_list_random = alphabetical_list_creation(list_powerdistance_random)
        input_list_similar = alphabetical_list_creation(list_powerdistance_similar)
        input_list_human = alphabetical_list_creation(list_powerdistance)

    elif (Hofstede_dimensions == 'Individualism vs Collectivism'):
        dim_index="Individualism index"
        dim_ranking="Individualism ranking"
        left_polar_list_random, right_polar_list_random = polar_list(list_individualism_random)
        left_polar_list_similar, right_polar_list_similar = polar_list(list_individualism_similar)
        left_polar_list_human, right_polar_list_human = polar_list(list_individualism)

        input_list_random = alphabetical_list_creation(list_individualism_random)
        input_list_similar = alphabetical_list_creation(list_individualism_similar)
        input_list_human = alphabetical_list_creation(list_individualism)

    elif (Hofstede_dimensions == 'Masculinity vs Femininity'):
        dim_index="Masculinity index"
        dim_ranking="Masculinity ranking"
        left_polar_list_random, right_polar_list_random = polar_list(list_masculinity_random)
        left_polar_list_similar, right_polar_list_similar = polar_list(list_masculinity_similar)
        left_polar_list_human, right_polar_list_human = polar_list(list_masculinity)

        input_list_random = alphabetical_list_creation(list_masculinity_random)
        input_list_similar = alphabetical_list_creation(list_masculinity_similar)
        input_list_human = alphabetical_list_creation(list_masculinity)

    elif (Hofstede_dimensions == 'Long Term vs Short Term Orientation'):
        dim_index="Long term orientation index"
        dim_ranking="Long term orientation ranking"
        left_polar_list_random, right_polar_list_random = polar_list(list_longterm_random)
        left_polar_list_similar, right_polar_list_similar = polar_list(list_longterm_similar)
        left_polar_list_human, right_polar_list_human = polar_list(list_longterm)

        input_list_random = alphabetical_list_creation(list_longterm_random)
        input_list_similar = alphabetical_list_creation(list_longterm_similar)
        input_list_human = alphabetical_list_creation(list_longterm)

    elif (Hofstede_dimensions == 'Indulgence vs Restraint'):
        dim_index="Indulgence index"
        dim_ranking="Indulgence ranking"
        left_polar_list_random, right_polar_list_random = polar_list(list_indulgence_random)
        left_polar_list_similar, right_polar_list_similar = polar_list(list_indulgence_similar)
        left_polar_list_human, right_polar_list_human = polar_list(list_indulgence)

        input_list_random = alphabetical_list_creation(list_indulgence_random)
        input_list_similar = alphabetical_list_creation(list_indulgence_similar)
        input_list_human = alphabetical_list_creation(list_indulgence)

    elif (Hofstede_dimensions == 'Uncertainty Avoidance'):
        dim_index="Uncertainty avoidance index"
        dim_ranking="Uncertainty avoidance ranking"
        left_polar_list_random, right_polar_list_random = polar_list(list_uncertainity_avoidance_random)
        left_polar_list_similar, right_polar_list_similar = polar_list(list_uncertainity_avoidance_similar)
        left_polar_list_human, right_polar_list_human = polar_list(list_uncertainity_avoidance)

        input_list_random = alphabetical_list_creation(list_uncertainity_avoidance_random)
        input_list_similar = alphabetical_list_creation(list_uncertainity_avoidance_similar)
        input_list_human = alphabetical_list_creation(list_uncertainity_avoidance)


    company_df = total_company_count_df.copy()  # This make a copy of data frame

    # Below lines will find the number of companies aligned to the respective left word in antonym pair
    company_df = company_count(company_df, input_list_random, polar_embedding)
    company_df = company_count(company_df, input_list_similar, polar_embedding)
    company_df = company_count(company_df, input_list_human, polar_embedding)

    # Below lines will find the total score based on the left word and final give a ranking
    company_df = polar_ranking(left_polar_list_random, "Total Score Random", "Polar Rank R", company_df)
    company_df = polar_ranking(left_polar_list_similar, "Total Score Similar", "Polar Rank S", company_df)
    company_df = polar_ranking(left_polar_list_human, "Total Score Human", "Polar Rank H", company_df)

    length = len(left_polar_list_random) + len(left_polar_list_similar) + len(left_polar_list_human)
    company_df.drop(company_df.iloc[:, 2:2 + (length) * 2], axis=1, inplace=True)

    hofstede_df = hofstede_df[hofstede_df.iloc[:, :] != "#NULL!"]
    hofstede_df.dropna(axis=0)

    # This merge the company dataframe and Hofstede dataframe over the common column Country
    merged_df = pd.merge(company_df, hofstede_df, how='left', on='Country')

    ranking_list = []
    for i in range(1, len(merged_df[dim_index]) + 1):
        ranking_list.append(i)
    merged_df = merged_df.sort_values(by=[dim_index], ascending=False)
    merged_df[dim_raking] = ranking_list

    fig = plt.figure(figsize=(10,4))
    sns.regplot(x=merged_df[dimension_ranking], y=merged_df["Polar Rank R"])
    sns.regplot(x=merged_df[dimension_ranking], y=merged_df["Polar Rank S"])
    sns.regplot(x=merged_df[dimension_ranking], y=merged_df["Polar Rank H"])
    st.pyplot(fig)




    # embeddings = st.sidebar.selectbox('Select pre-trained embbeddings?',('Wikipedia',  'Google News', 'Twitter', 'Reddit'))
    # if(embeddings == 'Wikipedia'):
    #     url = "https://docs.google.com/spreadsheets/d/1bkhvGSLMIKFHfbjdzH6LRunqYpt5x8-tUQvdutUCvaU/edit?usp=sharing"
    # elif(embeddings == 'Google News'):
    #     url = "https://docs.google.com/spreadsheets/d/1AiU_rhYuBphWByYszFXtFHDdGQ8yBXvqBvLEhuOEsgk/edit?usp=sharing"
    # elif(embeddings == 'Twitter'):
    #     url = "https://docs.google.com/spreadsheets/d/1dA6wI4ut8xhcDV0NRVdNq6hPKvgUhbWnroo_z2WFVf8/edit?usp=sharing"
    # elif(embeddings == 'Reddit'):
    #     url = "https://docs.google.com/spreadsheets/d/1uq5ZwkVIYBj7NLb0jtm2bEo-LZmO57TeZ-f4k8DMFsE/edit?usp=sharing"
    # conn = connect()
    # rows = conn.execute(f'SELECT * FROM "{url}"')
    # df_gsheet = pd.DataFrame(rows)
    # st.write(df_gsheet)
# st.write('You selected:', option